# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "requests<3",
#   "httpx",
#   "python-dotenv",
#   "fastapi",
#   "uvicorn"
# ]
# ///

import os
import asyncio
from fastapi import FastAPI, Request, HTTPException
from fastapi import BackgroundTasks
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
import httpx

load_dotenv()

AIPIPE_TOKEN=os.getenv('AIPIPE_TOKEN')
AIPIPE_URL='https://aipipe.org/openrouter/v1/chat/completions'
SECRET_KEY=os.getenv('SECRET_KEY')

app = FastAPI()
'''
{
  "email": "your email", // Student email ID
  "secret": "your secret", // Student-provided secret
  "url": "https://example.com/quiz-834" // A unique task URL
}
'''

async def process_request(data):


    # visit the url in data['url], it will have a quiz , read that quiz and do whatever it asks to do, have llm generate code from taking request(data) that came and till sending response to the submit url thats on page 
    prompt_for_llm = f"""
    You have been given a URL: {data.get("url")}. Generate a standalone python script that uses
    httpx to visit this URL, read the quiz question on the page, generate an appropriate answer
    using an LLM API (like AIPIPE) with the following token: {AIPIPE_TOKEN}, and then POST the
    answer back to the submission endpoint found on that page. Make sure to include all the
    necessary headers and handle any required JSON formatting for both the LLM request and the
    submission. The script should be fully functional and ready to run.nothing outside of the code should be in the response.
    """

    llm_response = httpx.post(
        AIPIPE_URL,
        headers={
            "accept": "*/*",
            "accept-language": "en-US,en;q=0.9",
            "authorization": f"Bearer {AIPIPE_TOKEN}",
            "content-type": "application/json",
        },
        json={
            "model": "openai/gpt-4.1-nano",
            "max_tokens": 1000,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that read quizes from webpages and generates python scripts to answer them."},
                {"role": "user", "content": prompt_for_llm}
            ]
        },timeout=60.0
    )
    llm_response_json = llm_response.json()
    # use subprocess to run the code generated by llm
    code_to_run = llm_response_json.get("choices",[])[0].get('message',{}).get('content','')
    print("Generated code to run:\n",code_to_run)
    # Save the generated code to a temporary file
    with open("temp_quiz_solver.py", "w") as f:
        f.write(code_to_run)
    # Run the generated code using subprocess
    import subprocess
    complete = subprocess.run(["python3", "temp_quiz_solver.py"],capture_output=True,env=os.environ.copy())    
    stdout = complete.stdout
    stderr = complete.stderr





    await asyncio.sleep(10)  #
    print("Processing request for:",data.get("email"))
    response = httpx.post('http://tds-llm-analysis.s-anand.net/submit',json={
        'email':data.get('email'),
        'secret':data.get('secret'),
        'url':data.get('url'),
        'answer':'anything you want'
    })
    print("Finished request for:",data.get("email"))

    print(response)
    print("Response text:", response.text)
    

@app.post("/receive_request")
async def receive_request(request: Request,background_tasks: BackgroundTasks):
    data = await request.json()
    if data.get("secret")!=SECRET_KEY:
        return JSONResponse(status_code=403,content={"message":"Forbidden"})
    else:
        background_tasks.add_task(process_request,data)
        return JSONResponse(status_code=200,content={"message":"Request accepted"})
    
if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host='127.0.0.1', port=8000)